{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIsKzLwBqbDr"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muLYFqfLqbDw"
      },
      "source": [
        "Если версия numpy отличается, то может возникнуть ошибка при обучении. Нужные версии есть в requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXUWVyH1w0Kr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from lightning import LightningModule, Trainer\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from madgrad import MADGRAD\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.normalizers import Lowercase\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from torchmetrics import AUROC, MaxMetric, MeanMetric\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "train = pd.read_parquet(\"data/train.parquet\")\n",
        "test = pd.read_parquet(\"data/test.parquet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq6ubcTrqbDz"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLoAjfsdw0K9"
      },
      "source": [
        "# TOKENIZER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yay9ub74w0K-"
      },
      "outputs": [],
      "source": [
        "train[\"ciphers_str\"] = train.ciphers.apply(lambda x: \" \".join(x))\n",
        "train[\"curves_str\"] = train.curves.apply(lambda x: \" \".join(x))\n",
        "\n",
        "test[\"ciphers\"] = (\n",
        "    test[\"ciphers\"].astype(str).str.slice(3, -2).str.replace('\"', \"\").str.split(\",\")\n",
        ")\n",
        "test[\"curves\"] = (\n",
        "    test[\"curves\"].astype(str).str.slice(3, -2).str.replace('\"', \"\").str.split(\",\")\n",
        ")\n",
        "test[\"ciphers_str\"] = test.ciphers.apply(lambda x: \" \".join(x))\n",
        "test[\"curves_str\"] = test.curves.apply(lambda x: \" \".join(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z4sPLcZw0LA"
      },
      "outputs": [],
      "source": [
        "def clean_ua(data):\n",
        "    \"\"\"\n",
        "    Removes unnecessary characters from a given dataframe column named \"ua\".\n",
        "\n",
        "    Args:\n",
        "    -   data: A pandas dataframe.\n",
        "\n",
        "    Returns:\n",
        "    -    A pandas dataframe with the cleaned ua column.\n",
        "    \"\"\"\n",
        "    data[\"ua_proc\"] = data[\"ua\"].apply(lambda x: re.sub(\"[()/;,]\", \"\", x))\n",
        "    data[\"ua_proc\"] = data[\"ua\"].str.partition('\"')[0]\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_tokenizer_curves(\n",
        "    train: pd.DataFrame, test: pd.DataFrame, min_frequency=10, vocab_size=2048\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains a Tokenizer model on the \"curves_str\" column of the given train and test dataframes.\n",
        "\n",
        "    Args:\n",
        "    -    train: A pandas dataframe containing the training data.\n",
        "    -    test: A pandas dataframe containing the testing data.\n",
        "    -    min_frequency: Minimum frequency of subwords. Defaults to 10.\n",
        "    -    vocab_size: Size of the final vocabulary. Defaults to 2048.\n",
        "\n",
        "    Returns:\n",
        "    -    A trained Tokenizer model.\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "    tokenizer.normalizer = Lowercase()\n",
        "\n",
        "    trainer = BpeTrainer(\n",
        "        special_tokens=[\"[PAD]\", \"[UNK]\", \" \"],\n",
        "        min_frequency=min_frequency,\n",
        "        vocab_size=vocab_size,\n",
        "    )\n",
        "\n",
        "    tokenizer.train_from_iterator(\n",
        "        [f\"{row.curves_str}\" for row in chain(train.itertuples(), test.itertuples())],\n",
        "        trainer=trainer,\n",
        "    )\n",
        "    tokenizer.enable_padding()\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def get_tokenizer_ua(\n",
        "    train: pd.DataFrame, test: pd.DataFrame, min_frequency=10, vocab_size=2048\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains a Tokenizer model on the \"ua\" column of the given train and test dataframes.\n",
        "\n",
        "    Args:\n",
        "    -    train: A pandas dataframe containing the training data.\n",
        "    -    test: A pandas dataframe containing the testing data.\n",
        "    -    min_frequency: Minimum frequency of subwords. Defaults to 10.\n",
        "    -    vocab_size: Size of the final vocabulary. Defaults to 2048.\n",
        "\n",
        "    Returns:\n",
        "    -    A trained Tokenizer model.\n",
        "    \"\"\"\n",
        "    clean_ua(train)\n",
        "    clean_ua(test)\n",
        "\n",
        "    tokenizer_ua = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "    tokenizer_ua.normalizer = Lowercase()\n",
        "\n",
        "    trainer = BpeTrainer(\n",
        "        special_tokens=[\"[PAD]\", \"[UNK]\", \" \"],\n",
        "        min_frequency=min_frequency,\n",
        "        vocab_size=vocab_size,\n",
        "    )\n",
        "\n",
        "    tokenizer_ua.train_from_iterator(\n",
        "        [f\"{row.ua_proc}\" for row in chain(train.itertuples(), test.itertuples())],\n",
        "        trainer=trainer,\n",
        "    )\n",
        "    tokenizer_ua.enable_padding()\n",
        "\n",
        "    return tokenizer_ua\n",
        "\n",
        "\n",
        "def get_tokenizer_ciphers(\n",
        "    train: pd.DataFrame, test: pd.DataFrame, min_frequency=10, vocab_size=2048\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains a Tokenizer model on the \"ciphers_str\" column of the given train and test dataframes.\n",
        "\n",
        "    Args:\n",
        "    -    train: A pandas dataframe containing the training data.\n",
        "    -    test: A pandas dataframe containing the testing data.\n",
        "    -    min_frequency: Minimum frequency of subwords. Defaults to 10.\n",
        "    -    vocab_size: Size of the final vocabulary. Defaults to 2048.\n",
        "\n",
        "    Returns:\n",
        "    -    A trained Tokenizer model.\n",
        "    \"\"\"\n",
        "    tokenizer_ciphers = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "    tokenizer_ciphers.normalizer = Lowercase()\n",
        "\n",
        "    trainer = BpeTrainer(\n",
        "        special_tokens=[\"[PAD]\", \"[UNK]\", \" \"],\n",
        "        min_frequency=min_frequency,\n",
        "        vocab_size=vocab_size,\n",
        "    )\n",
        "\n",
        "    tokenizer_ciphers.train_from_iterator(\n",
        "        [f\"{row.ciphers_str}\" for row in chain(train.itertuples(), test.itertuples())],\n",
        "        trainer=trainer,\n",
        "    )\n",
        "    tokenizer_ciphers.enable_padding()\n",
        "\n",
        "    return tokenizer_ciphers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSnhER8Jw0LB",
        "outputId": "43aa1d01-651a-4569-928b-c6be92a56f2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer_ciphers = get_tokenizer_ciphers(train, test)\n",
        "tokenizer_curves = get_tokenizer_curves(train, test)\n",
        "tokenizer_ua = get_tokenizer_ua(train, test)\n",
        "\n",
        "CIPHERS_PADDING_IDX = tokenizer_ciphers.token_to_id(\"[PAD]\")\n",
        "CIPHERS_VOCAB_SIZE = tokenizer_ciphers.get_vocab_size()\n",
        "\n",
        "CURVES_PADDING_IDX = tokenizer_curves.token_to_id(\"[PAD]\")\n",
        "CURVES_VOCAB_SIZE = tokenizer_curves.get_vocab_size()\n",
        "\n",
        "UA_PADDING_IDX = tokenizer_ua.token_to_id(\"[PAD]\")\n",
        "UA_VOCAB_SIZE = tokenizer_ua.get_vocab_size()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2QWitifqbD4"
      },
      "source": [
        "# DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaDA6HxJw0LD"
      },
      "outputs": [],
      "source": [
        "class TrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data: pd.DataFrame) -> None:\n",
        "        self.data = data\n",
        "        self.data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[int, str, int]:\n",
        "        row = self.data.loc[idx]\n",
        "        return row.ciphers_str, row.curves_str, row.ua_proc, row.label\n",
        "\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data: pd.DataFrame) -> None:\n",
        "        self.data = data\n",
        "        self.data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[int, str, int]:\n",
        "        row = self.data.loc[idx]\n",
        "        return row.ciphers_str, row.curves_str, row.ua_proc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1m1pP2mw0LC"
      },
      "outputs": [],
      "source": [
        "def tokenize(texts: List[str], tokenizer: str) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Tokenizes a list of text using given tokenizer model.\n",
        "\n",
        "    Args:\n",
        "    -    texts: A list of strings to tokenize.\n",
        "    -    tokenizer: A string specifying the tokenizer to use.\n",
        "\n",
        "    Returns:\n",
        "    -    A torch tensor representing the tokenized texts.\n",
        "    \"\"\"\n",
        "    return torch.tensor(\n",
        "        [x.ids for x in tokenizer.encode_batch(texts, add_special_tokens=True)]\n",
        "    )\n",
        "\n",
        "\n",
        "def collate_to_train_batch(\n",
        "    batch: List[Tuple[str, str, str, int]]\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Collate a batch of training data.\n",
        "\n",
        "    Args:\n",
        "    -    batch: a list of tuples representing the train data to collate.\n",
        "\n",
        "    Returns:\n",
        "    -    A tuple of torch tensors representing the train collated data.\n",
        "    \"\"\"\n",
        "    ciphers, curves, ua, labels = zip(*batch)\n",
        "\n",
        "    ciphers_tensor = tokenize(ciphers, tokenizer=tokenizer_ciphers)\n",
        "    curves_tensor = tokenize(curves, tokenizer=tokenizer_curves)\n",
        "    ua_tensor = tokenize(ua, tokenizer=tokenizer_ua)\n",
        "    label_tensor = torch.Tensor(labels).view(-1, 1)\n",
        "\n",
        "    return (ciphers_tensor, curves_tensor, ua_tensor), label_tensor\n",
        "\n",
        "\n",
        "def collate_to_test_batch(\n",
        "    batch: List[Tuple[str, str, str]]\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Collate a batch of training data.\n",
        "\n",
        "     Args:\n",
        "    -    batch: a list of tuples representing the test data to collate.\n",
        "\n",
        "    Returns:\n",
        "    -    A tuple of torch tensors representing the collated test data.\n",
        "    \"\"\"\n",
        "    ciphers, curves, ua = zip(*batch)\n",
        "\n",
        "    ciphers_tensor = tokenize(ciphers, tokenizer=tokenizer_ciphers)\n",
        "    curves_tensor = tokenize(curves, tokenizer=tokenizer_curves)\n",
        "    ua_tensor = tokenize(ua, tokenizer=tokenizer_ua)\n",
        "\n",
        "    return ciphers_tensor, curves_tensor, ua_tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHyoZPkmqbD5"
      },
      "source": [
        "# MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKxlnCD2qbD5"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    Examples:\n",
        "        >>> pos_encoder = PositionalEncoding(d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [batch size, sequence length, embed dim]\n",
        "            output: [batch size, sequence length, embed dim]\n",
        "        Examples:\n",
        "            >>> output = pos_encoder(x)\n",
        "        \"\"\"\n",
        "        t = self.pe.permute(1, 0, 2)[\n",
        "            :,\n",
        "            : x.size(1),\n",
        "        ]\n",
        "        x = x + t\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder\n",
        "\n",
        "    Args:\n",
        "        ntoken (int): vocabulary size (num_embeddings in Embedding)\n",
        "        ninp (int): embedding size (embedding_dim in Embedding)\n",
        "        nhead (int): number of heads in TransformerEncoderLayer\n",
        "        nhid (int): feedforward size in TransformerEncoderLayer\n",
        "        noutp (int): output size\n",
        "        nlayers (int): number of TransformerEncoderLayer\n",
        "        padding_idx (int): padding token\n",
        "        dropout (float, optional): dropout for TransformerEncoderLayer. Defaults to 0.5.\n",
        "        pos_max_len (int, optional): maximum sequence length for positional encoding. Defaults to 700.\n",
        "\n",
        "    Raises:\n",
        "        ImportError: ImportError\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        ntoken: int,\n",
        "        ninp: int,\n",
        "        nhead: int,\n",
        "        nhid: int,\n",
        "        noutp: int,\n",
        "        nlayers: int,\n",
        "        padding_idx: int,\n",
        "        dropout: float = 0.5,\n",
        "        pos_max_len: int = 700,\n",
        "    ):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        try:\n",
        "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        except BaseException as e:\n",
        "            raise ImportError(\n",
        "                \"TransformerEncoder module does not exist in PyTorch 1.1 or \" \"lower.\"\n",
        "            ) from e\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout, pos_max_len)\n",
        "        encoder_layers = TransformerEncoderLayer(\n",
        "            ninp, nhead, nhid, dropout, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp, padding_idx=padding_idx)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, noutp)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = (\n",
        "            mask.float()\n",
        "            .masked_fill(mask == 0, float(\"-inf\"))\n",
        "            .masked_fill(mask == 1, float(0.0))\n",
        "        )\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, has_mask: bool = False) -> torch.Tensor:\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            src: the sequence for processing.\n",
        "        Shape:\n",
        "            src: [batch size, sequence length, ninp]\n",
        "            output: [batch size, sequence length, noutp]\n",
        "        \"\"\"\n",
        "        if has_mask:\n",
        "            device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        src = self.encoder(src) * np.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfPbiwDEw0LH"
      },
      "outputs": [],
      "source": [
        "class SubModel(nn.Module):\n",
        "    \"\"\"SubModel for handling each part of input data separately, based on Transformer\n",
        "\n",
        "    Args:\n",
        "        padding_idx (int): padding token\n",
        "        vocab_size (int): vocabulary size\n",
        "        embed_size (int): embedding size\n",
        "        hidden_size (int): output size\n",
        "        pos_max_len (int): maximum sequence length for positional encoding\n",
        "        dropout (float): dropout for Transformer\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        padding_idx: int,\n",
        "        vocab_size: int,\n",
        "        embed_size: int,\n",
        "        hidden_size: int,\n",
        "        pos_max_len: int,\n",
        "        dropout: float,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.transformer = TransformerModel(\n",
        "            ntoken=vocab_size,\n",
        "            ninp=embed_size,\n",
        "            nhead=2,\n",
        "            nhid=256,\n",
        "            noutp=hidden_size,\n",
        "            nlayers=3,\n",
        "            padding_idx=padding_idx,\n",
        "            dropout=dropout,\n",
        "            pos_max_len=pos_max_len,\n",
        "        )\n",
        "\n",
        "    def get_embed(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        embeds = self.transformer(tensor)\n",
        "        embed_max = torch.nn.functional.max_pool2d(\n",
        "            embeds, kernel_size=(embeds.size(1), 1)\n",
        "        ).squeeze(dim=1)\n",
        "        return embed_max - embeds.mean(dim=1)\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            src: the sequence for processing.\n",
        "        Shape:\n",
        "            src: [batch size, sequence length]\n",
        "            output: [batch size, sequence length, hidden_size]\n",
        "        \"\"\"\n",
        "        embeds = self.get_embed(tensor)\n",
        "        return embeds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UhYufyfw0LI"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_ciphers: int,\n",
        "        vocab_curves: int,\n",
        "        vocab_ua: int,\n",
        "        length_ciphers: int,\n",
        "        length_curves: int,\n",
        "        length_ua: int,\n",
        "        hidden_size: int,\n",
        "        pad_ciphers: int,\n",
        "        pad_curves: int,\n",
        "        pad_ua: int,\n",
        "    ):\n",
        "        \"\"\"Main Model\n",
        "\n",
        "        Args:\n",
        "            vocab_ciphers (int): vocabulary size for ciphers\n",
        "            vocab_curves (int): vocabulary size for ciphers\n",
        "            vocab_ua (int): vocabulary size for ciphers\n",
        "            length_ciphers (int): maximum sequence length for positional encoding for ciphers\n",
        "            length_curves (int): maximum sequence length for positional encoding for curves\n",
        "            length_ua (int): maximum sequence length for positional encoding for ua\n",
        "            hidden_size (int): hidden size for fc\n",
        "            pad_ciphers (int): padding token for ciphers\n",
        "            pad_curves (int): padding token for ciphers\n",
        "            pad_ua (int): padding token for ciphers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embed_ciphers = SubModel(\n",
        "            vocab_size=vocab_ciphers,\n",
        "            embed_size=128,\n",
        "            hidden_size=64,\n",
        "            padding_idx=pad_ciphers,\n",
        "            pos_max_len=length_ciphers,\n",
        "            dropout=0.1,\n",
        "        ) # на выходе эмбеддинг шифров\n",
        "        self.embed_curves = SubModel(\n",
        "            vocab_size=vocab_curves,\n",
        "            embed_size=128,\n",
        "            hidden_size=64,\n",
        "            padding_idx=pad_curves,\n",
        "            pos_max_len=length_curves,\n",
        "            dropout=0.1,\n",
        "        ) # на выходе эмбеддинг кривых\n",
        "        self.embed_ua = SubModel(\n",
        "            vocab_size=vocab_ua,\n",
        "            embed_size=128,\n",
        "            hidden_size=64,\n",
        "            padding_idx=pad_ua,\n",
        "            pos_max_len=length_ua,\n",
        "            dropout=0.1,\n",
        "        ) # на выходе эмбеддинг юзер агента\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(64 * 3, hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.BatchNorm1d(hidden_size // 2),\n",
        "        ) # на выходе получаем финальный TLS эмбеддинг\n",
        "        self.activation = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size // 2, 1) # финальный классификатор\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            src: the sequence for processing.\n",
        "        Shape:\n",
        "            src: [batch size, sequence length]\n",
        "            output: [batch size, 1]\n",
        "        \"\"\"\n",
        "        embed_ciphers = self.embed_ciphers(x[0])\n",
        "        embed_curves = self.embed_curves(x[1])\n",
        "        embed_ua = self.embed_ua(x[2])\n",
        "        embed = torch.cat((embed_ciphers, embed_curves, embed_ua), dim=1)\n",
        "\n",
        "        outp = self.fc1(embed)\n",
        "        outp = self.fc2(self.activation(outp))\n",
        "        return outp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3CUFYW2w0LJ"
      },
      "outputs": [],
      "source": [
        "class LitModule(LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        net: torch.nn.Module,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(logger=False)\n",
        "\n",
        "        self.net = net\n",
        "        self.criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([0.5]))\n",
        "\n",
        "        self.train_rocauc = AUROC(\n",
        "            task=\"binary\",\n",
        "        )\n",
        "        self.val_rocauc = AUROC(\n",
        "            task=\"binary\",\n",
        "        )\n",
        "        self.train_loss = MeanMetric()\n",
        "        self.val_loss = MeanMetric()\n",
        "        self.val_rocauc_best = MaxMetric()\n",
        "\n",
        "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
        "        optim = MADGRAD(self.parameters(), lr=0.0001)\n",
        "        self.sch = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9)\n",
        "        return optim\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.net(x)\n",
        "\n",
        "    def on_train_start(self):\n",
        "        self.val_loss.reset()\n",
        "        self.val_rocauc.reset()\n",
        "        self.val_rocauc_best.reset()\n",
        "\n",
        "    def model_step(self, batch):\n",
        "        x, y = batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = torch.sigmoid(logits)\n",
        "        return loss, preds.squeeze(), y.squeeze()\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        loss, preds, targets = self.model_step(batch)\n",
        "\n",
        "        self.train_loss(loss)\n",
        "        self.train_rocauc(preds, targets)\n",
        "        self.log(\n",
        "            \"train_loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True\n",
        "        )\n",
        "        self.log(\n",
        "            \"train_rocauc\",\n",
        "            self.train_rocauc,\n",
        "            on_step=False,\n",
        "            on_epoch=True,\n",
        "            prog_bar=True,\n",
        "        )\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        self.sch.step()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, preds, targets = self.model_step(batch)\n",
        "\n",
        "        self.val_loss(loss)\n",
        "        self.val_rocauc(preds, targets)\n",
        "        self.log(\"val_loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        self.log(\n",
        "            \"val_rocauc\", self.val_rocauc, on_step=False, on_epoch=True, prog_bar=True\n",
        "        )\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        rocauc = self.val_rocauc.compute()\n",
        "        self.val_rocauc_best(rocauc)\n",
        "        self.log(\"val_rocauc_best\", self.val_rocauc_best.compute(), prog_bar=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqlUoMQVqbD7"
      },
      "source": [
        "# TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOtofcFsqbD7"
      },
      "outputs": [],
      "source": [
        "skf = StratifiedKFold(n_splits=5)\n",
        "for i, (train_index, val_index) in enumerate(skf.split(train, train.label)):\n",
        "    print(f\"FOLD {i}\", \"-\" * 20)\n",
        "    train_fold = train.loc[train_index]\n",
        "    val_fold = train.loc[val_index]\n",
        "\n",
        "    train_dl = torch.utils.data.DataLoader(\n",
        "        TrainDataset(train_fold),\n",
        "        batch_size=128,\n",
        "        collate_fn=collate_to_train_batch,\n",
        "        pin_memory=False,\n",
        "        shuffle=True,\n",
        "    )\n",
        "    val_dl = torch.utils.data.DataLoader(\n",
        "        TrainDataset(val_fold),\n",
        "        batch_size=128,\n",
        "        collate_fn=collate_to_train_batch,\n",
        "        pin_memory=False,\n",
        "    )\n",
        "\n",
        "    model = LitModule(\n",
        "        Model(\n",
        "            length_ciphers=248,\n",
        "            length_curves=65,\n",
        "            length_ua=156,\n",
        "            hidden_size=256,\n",
        "            vocab_ciphers=CIPHERS_VOCAB_SIZE,\n",
        "            vocab_curves=CURVES_VOCAB_SIZE,\n",
        "            vocab_ua=UA_VOCAB_SIZE,\n",
        "            pad_ciphers=CIPHERS_PADDING_IDX,\n",
        "            pad_curves=CURVES_PADDING_IDX,\n",
        "            pad_ua=UA_PADDING_IDX,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor=\"val_loss\",\n",
        "        dirpath=\"models/37/\",\n",
        "        filename=f\"model-{i}\" + \"-{val_loss:.3f}\",\n",
        "        save_top_k=1,\n",
        "        mode=\"min\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=16,\n",
        "        accelerator=device,\n",
        "        callbacks=[checkpoint_callback],\n",
        "        enable_model_summary=False,\n",
        "    )\n",
        "    trainer.fit(model=model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yjkexc9sqbD7"
      },
      "source": [
        "# SUBMISSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xJDLcKlqbD7"
      },
      "outputs": [],
      "source": [
        "model_paths = Path(\"models/37/\").iterdir()\n",
        "all_probs = []\n",
        "for model_path in model_paths:\n",
        "    cur_model = LitModule.load_from_checkpoint(model_path)\n",
        "\n",
        "    test_dl = torch.utils.data.DataLoader(\n",
        "        TestDataset(test),\n",
        "        batch_size=128,\n",
        "        collate_fn=collate_to_test_batch,\n",
        "        pin_memory=False,\n",
        "        shuffle=False,\n",
        "    )\n",
        "    probs = torch.concat(trainer.predict(model, dataloaders=test_dl)).numpy()\n",
        "    all_probs.append(probs)\n",
        "all_probs = np.array(all_probs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "u_c2sOaRCjdP",
        "outputId": "a443ec7c-fe82-4065-c2ee-36fb41063c75"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>is_bot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>0.907719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>0.565657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>0.904699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20</td>\n",
              "      <td>0.618101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21</td>\n",
              "      <td>0.565810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14384</th>\n",
              "      <td>62317</td>\n",
              "      <td>0.286175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14385</th>\n",
              "      <td>62319</td>\n",
              "      <td>0.946781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14386</th>\n",
              "      <td>62322</td>\n",
              "      <td>0.874187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14387</th>\n",
              "      <td>62333</td>\n",
              "      <td>0.997575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14388</th>\n",
              "      <td>62335</td>\n",
              "      <td>0.598644</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14389 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id    is_bot\n",
              "0          5  0.907719\n",
              "1          6  0.565657\n",
              "2         12  0.904699\n",
              "3         20  0.618101\n",
              "4         21  0.565810\n",
              "...      ...       ...\n",
              "14384  62317  0.286175\n",
              "14385  62319  0.946781\n",
              "14386  62322  0.874187\n",
              "14387  62333  0.997575\n",
              "14388  62335  0.598644\n",
              "\n",
              "[14389 rows x 2 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subm = pd.DataFrame(\n",
        "    {\n",
        "        \"id\": test.id,\n",
        "        \"is_bot\": torch.sigmoid(torch.tensor(all_probs))\n",
        "        .mean(dim=0)\n",
        "        .squeeze(dim=1)\n",
        "        .numpy(),\n",
        "    }\n",
        ")\n",
        "subm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vNc8GezC_jm"
      },
      "outputs": [],
      "source": [
        "subm.to_csv(\"subm37.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "af001c29e8c8ba6225572cf219a918f005bf3d5803e1ec722518eee1139734b4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}