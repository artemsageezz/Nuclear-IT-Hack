{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sage-artem/Nuclear-IT-Hack/blob/main/vk-task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWERfHhx9OT4"
      },
      "outputs": [],
      "source": [
        "pip install torch tokenizers pandas fastparquet pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "_ZlbL3Ha9Srp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "from typing import List, Tuple, Union\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.normalizers import Lowercase\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import xgboost as xgb\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from multiprocessing import freeze_support, Process"
      ],
      "metadata": {
        "id": "lAd0aqs59T4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def worker():\n",
        "    print('Worker')\n",
        "\n",
        "\n",
        "def pars(x):\n",
        "    y = []\n",
        "    for d in x:\n",
        "        k = 0\n",
        "        i1 = d.find('(')\n",
        "        i2 = d.find(')') + 1\n",
        "        xz = d[i1:i2].replace('(', '').replace(')', '')\n",
        "        d = d[:i1-1] + d[i2:]\n",
        "        i1 = d.find('(')\n",
        "        i2 = d.find(')') + 1\n",
        "        xz1 = d[i1:i2].replace('(', '').replace(')', '')\n",
        "        if i1 == -1:\n",
        "            k = 1\n",
        "        else:\n",
        "            d = d[:i1-1] + d[i2:]\n",
        "        d = d.split(' ')\n",
        "        d.insert(1, xz)\n",
        "        if k == 1:\n",
        "            pass\n",
        "        else:\n",
        "            d.insert(3, xz1)\n",
        "        y.append(d)\n",
        "    return y\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    freeze_support()\n",
        "    p = Process(target=worker)\n",
        "    p.start()\n",
        "    p.join()"
      ],
      "metadata": {
        "id": "Hi8uJqGX9VUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "tokenizerROBERTA = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "roberta_model = RobertaModel.from_pretrained('roberta-base')\n",
        "text = [\"Replace me by any text you'd like.\", 'Hello, my name is Artem', 'Maching Learning is cool']\n",
        "encoded_input = tokenizerROBERTA(text, return_tensors='pt', padding=True)\n",
        "output = roberta_model(**encoded_input)"
      ],
      "metadata": {
        "id": "Te8oOO_f9Yxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in output:\n",
        "  print(output[t].shape)"
      ],
      "metadata": {
        "id": "riJBWWKCXLuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input"
      ],
      "metadata": {
        "id": "NXzNq2qgAK6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "abI4D7MT-WgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tOxL0EbB-T4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled = pd.DataFrame({'ua': [],\n",
        "                          'ciphers': [],\n",
        "                          'curves': []})\n",
        "unlabeled"
      ],
      "metadata": {
        "id": "yvwuFevR-Zkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = 100000\n",
        "chunk_size = 1000\n",
        "unlabeled = pd.DataFrame()\n",
        "counter = 0\n",
        "parquet_file = pq.ParquetFile('/content/drive/MyDrive/unlabelled.snappy.parquet')\n",
        "for i in parquet_file.iter_batches(batch_size=chunk_size):\n",
        "    counter += 1\n",
        "    unlabeled = pd.concat([unlabeled, i.to_pandas()], ignore_index=True)\n",
        "    if counter == num_examples // chunk_size:\n",
        "        break"
      ],
      "metadata": {
        "id": "nBBZ-rTk-a5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_parquet('/content/train.parquet')\n",
        "test = pd.read_parquet('/content/test.parquet')"
      ],
      "metadata": {
        "id": "LE3zar6U-dEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['ua'] = pars(train['ua'].tolist())\n",
        "test['ua'] = pars(test['ua'].tolist())\n",
        "#unlabeled['ua'] = pars(unlabeled['ua'].tolist())"
      ],
      "metadata": {
        "id": "jY_WF1jL-e6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_nebot = train[train.label == 0]\n",
        "train_nebot = pd.concat([train_nebot, train_nebot])\n",
        "\n",
        "for i in range(1, train_nebot.shape[0]+1):\n",
        "    train_nebot.iloc[i-1, 0] = i+62350\n",
        "\n",
        "train = pd.concat([train, train_nebot])\n",
        "train = shuffle(train)"
      ],
      "metadata": {
        "id": "LWOw5aMb-zYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = Lowercase()\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[SEP]\"], vocab_size=320)\n",
        "\n",
        "tokenizer.train_from_iterator(\n",
        "    [f\"{row.ciphers} [SEP] {row.curves}\" for row in chain(train.itertuples(), test.itertuples())],\n",
        "    trainer=trainer\n",
        ")\n",
        "tokenizer.enable_padding()\n",
        "\n",
        "PADDING_IDX = tokenizer.token_to_id(\"[PAD]\")\n",
        "VOCAB_SIZE = tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "Zrmov1sDA6HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.array(tokenizerROBERTA(train['ua'].iloc[0], padding=True)['input_ids']))"
      ],
      "metadata": {
        "id": "LFE9Um9_DF6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train['ua'].iloc[0])\n",
        "# print(train['ciphers'].iloc[0])\n",
        "# spis = []\n",
        "# for row in chain(train.itertuples(), test.itertuples()):\n",
        "#     spis.append(np.array(tokenizerROBERTA(row.ua, padding=True)['input_ids']).shape[1])\n",
        "# print(max(spis))\n",
        "# print(tokenizerROBERTA(train['ua'].iloc[0]))"
      ],
      "metadata": {
        "id": "k-m5_fECCoMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data: pd.DataFrame) -> None:\n",
        "        self.data = data\n",
        "        self.data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[int, str, str]:\n",
        "        # Forget about UA for now\n",
        "        row = self.data.loc[idx]\n",
        "        return row.id, f\"{row.ciphers} [SEP] {row.curves}\", f'{row.ua}'"
      ],
      "metadata": {
        "id": "KKKpQEoW_HlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data: pd.DataFrame) -> None:\n",
        "        self.data = data\n",
        "        self.data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[int, str, int, str]:\n",
        "        # Forget about UA for now\n",
        "        row = self.data.loc[idx]\n",
        "        return row.id, f\"{row.ciphers} [SEP] {row.curves}\", row.label, f'{row.ua}'"
      ],
      "metadata": {
        "id": "mau5PR4y_KOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(texts: List[str]) -> torch.Tensor:\n",
        "    return torch.tensor([\n",
        "        _.ids + [0]*(830-len(_.ids)) for _ in tokenizer.encode_batch(texts, add_special_tokens=True)\n",
        "    ])\n",
        "\n",
        "def tokenizeROBERTA(texts: List[str]) -> torch.Tensor:\n",
        "    return {'input_ids': torch.tensor([\n",
        "        _ + [1]*(122-len(_)) for _ in tokenizerROBERTA(texts, padding=True)['input_ids']\n",
        "             ]),\n",
        "             'attention_mask': torch.tensor([_ for _ in tokenizerROBERTA(texts, padding=True)['attention_mask']])\n",
        "            }"
      ],
      "metadata": {
        "id": "VViJrefP_OYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_to_train_batch(batch: List[Tuple[int, str, int, str]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    ids, texts, labels, ua_texts = zip(*batch)\n",
        "\n",
        "    ids_tensor = torch.tensor(ids, dtype=torch.long).view(-1, 1)\n",
        "    texts_tensor = tokenize(texts)\n",
        "    ua_texts_tenzor = tokenizeROBERTA(ua_texts)\n",
        "    label_tensor = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
        "\n",
        "    return ids_tensor, texts_tensor, label_tensor, ua_texts_tenzor"
      ],
      "metadata": {
        "id": "IAdE2OKH_QRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_to_test_batch(batch: List[Tuple[int, str, str]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    ids, texts, ua_texts = zip(*batch)\n",
        "\n",
        "    ids_tensor = torch.tensor(ids, dtype=torch.long).view(-1, 1)\n",
        "    texts_tensor = tokenize(texts)\n",
        "    ua_texts_tenzor = tokenizeROBERTA(ua_texts)\n",
        "\n",
        "    return ids_tensor, texts_tensor, ua_texts_tenzor"
      ],
      "metadata": {
        "id": "Fi3Ysr-H_RPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = torch.utils.data.DataLoader(\n",
        "    TrainDataset(train), batch_size=128, num_workers=0, collate_fn=collate_to_train_batch, pin_memory=False\n",
        ")"
      ],
      "metadata": {
        "id": "TBJF5Tih_S4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dl = torch.utils.data.DataLoader(\n",
        "    TestDataset(test), batch_size=128, num_workers=0, collate_fn=collate_to_test_batch, pin_memory=False\n",
        ")"
      ],
      "metadata": {
        "id": "uB8IrVT1_T11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, padding_idx: int, vocab_size: int, embed_size: int, hidden_size: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emded_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # initialize embedding layers\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size, padding_idx=padding_idx)\n",
        "        #self.ua_embedding = nn.Embedding(num_embeddings=vocab_sizeua, embedding_dim=embed_size, padding_idx=padding_idxua)\n",
        "\n",
        "        # attention layers\n",
        "        self.attention1 = nn.MultiheadAttention(embed_size, 4)\n",
        "        self.linear1 = nn.Linear(embed_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.attention2 = nn.MultiheadAttention(hidden_size, 4)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linearUA = nn.Linear(768, hidden_size)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # hidden layers\n",
        "        self.hidden = nn.Sequential(\n",
        "            nn.Linear(hidden_size*(122 + 830), hidden_size),  # concatenate UA and TLS embeddings\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "        )\n",
        "\n",
        "        # classification layer\n",
        "        self.clf = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def get_embeds(self, tensor: torch.Tensor, ua_tensor: dict) -> torch.Tensor:\n",
        "        tensor = tensor.to(device)\n",
        "        tensor = tensor.transpose(0, 1)\n",
        "        for key in ua_tensor:\n",
        "            ua_tensor[key] = ua_tensor[key].to(device)\n",
        "            print(\"ua_tensor device:\", ua_tensor[key].device)\n",
        "        print(\"tensor device:\", tensor.device)\n",
        "\n",
        "        #ua_tensor = ua_tensor.to(device)\n",
        "        # ua_tensor = ua_tensor.transpose(0, 1)\n",
        "\n",
        "        # get embeddings for TLS\n",
        "        tls_embeds = self.embedding(tensor.to(device))\n",
        "        print(tls_embeds, tls_embeds.shape)\n",
        "        tls_embeds, _ = self.attention1(tls_embeds, tls_embeds, tls_embeds)\n",
        "        tls_embeds = self.relu(tls_embeds)\n",
        "        tls_embeds = self.linear1(tls_embeds)\n",
        "        tls_embeds = self.relu(tls_embeds)\n",
        "        tls_embeds, _ = self.attention2(tls_embeds, tls_embeds, tls_embeds)\n",
        "        tls_embeds = self.relu(tls_embeds)\n",
        "        tls_embeds = self.linear2(tls_embeds)\n",
        "        tls_embeds = torch.flatten(tls_embeds.transpose(0, 1), start_dim=1)\n",
        "\n",
        "        # get embeddings for User-Agent\n",
        "        tls_embeds_ua = roberta_model(**ua_tensor)[1]\n",
        "        print(tls_embeds_ua, tls_embeds_ua.shape)\n",
        "        tls_embeds_ua = self.linearUA(tls_embeds_ua)\n",
        "        print(tls_embeds_ua, tls_embeds_ua.shape)\n",
        "        tls_embeds_ua = torch.flatten(tls_embeds_ua, start_dim=1)\n",
        "        print(tls_embeds_ua, tls_embeds_ua.shape)\n",
        "\n",
        "        # concatenate embeddings\n",
        "        embeds = torch.cat((tls_embeds_ua, tls_embeds), dim=1)\n",
        "\n",
        "        return tls_embeds_ua\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor, ua_tensor: dict) -> torch.Tensor:\n",
        "        embeds = self.dropout(self.get_embeds(tensor, ua_tensor))\n",
        "        hiddens = self.hidden(embeds)\n",
        "        return self.clf(hiddens)"
      ],
      "metadata": {
        "id": "_5ivHGKV_Wrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LightningModel(pl.LightningModule):\n",
        "    def __init__(self, model) -> None:\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def training_step(self, batch: torch.Tensor) -> torch.Tensor:\n",
        "        _, X, y, X1 = batch\n",
        "        X = X.to(device)\n",
        "        for key in X1:\n",
        "            X1[key] = X1[key].to(device)\n",
        "        #X1 = X1.to(device)\n",
        "        return self.criterion(self.model(X, X1), y)\n",
        "\n",
        "    def predict_step(self, batch: torch.Tensor, _) -> torch.Tensor:\n",
        "        ids, X, X1, *_ = batch\n",
        "        X = X.to(device)\n",
        "        #X1 = X1.to(device)\n",
        "        for key in X1:\n",
        "            X1[key] = X1[key].to(device)\n",
        "        return ids, torch.sigmoid(self.model(X, X1))\n",
        "\n",
        "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
        "        return torch.optim.AdamW(self.parameters(), lr=0.005, weight_decay=0.05)\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(tensor)"
      ],
      "metadata": {
        "id": "skegJCbV_cAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_model = roberta_model.to(device)"
      ],
      "metadata": {
        "id": "pq8urJ61Une1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n",
        "ua_tensor = {\n",
        "            'input_ids': torch.LongTensor([[10, 11, 12], [13, 14, 15]]),\n",
        "            'attention_mask': torch.LongTensor([[1, 1, 0], [0, 1, 1]])\n",
        "}"
      ],
      "metadata": {
        "id": "dKrY8Ak2TIiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =  Model(vocab_size=VOCAB_SIZE, embed_size=64, hidden_size=48, padding_idx=PADDING_IDX, dropout=0.1)\n",
        "\n",
        "model = model.to(device)\n",
        "for key in ua_tensor:\n",
        "  ua_tensor[key] = ua_tensor[key].to(device)"
      ],
      "metadata": {
        "id": "HzxlU8V7T_Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.get_embeds(tensor.to(device), ua_tensor)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "K9RS4i-zTEvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LightningModel(\n",
        "    Model(vocab_size=VOCAB_SIZE, embed_size=64, hidden_size=48, padding_idx=PADDING_IDX, dropout=0.1)\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=1)\n",
        "trainer.fit(model=model, train_dataloaders=train_dl)"
      ],
      "metadata": {
        "id": "h7FNUN4y_d_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids, probs = zip(*trainer.predict(model, dataloaders=test_dl))\n",
        "\n",
        "(\n",
        "    pd.DataFrame({\n",
        "        \"id\": torch.concat(ids).squeeze().numpy(),\n",
        "        \"is_bot\": torch.concat(probs).squeeze().numpy()\n",
        "    })\n",
        "    .to_csv(\"baseline_submission_chas145.csv\", index=None)\n",
        ")"
      ],
      "metadata": {
        "id": "cLyfo-sg_fO-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}